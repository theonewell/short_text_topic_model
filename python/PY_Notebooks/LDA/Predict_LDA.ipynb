{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/theo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pprint for easier logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable logging for gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"LDA_Model/LDA.model\"\n",
    "lda_model = gensim.models.ldamodel.LdaModel.load(file_dir)\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentence):\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)  \n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the new text/document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatUnseen(unseen_document):\n",
    "    data_words = list(sent_to_words(unseen_document))\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(sent_to_words(data_words))\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def getTopTopic(unseen_document):\n",
    "    data_lemmatized = formatUnseen(unseen_document)\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    # Create Corpus\n",
    "    other_texts = data_lemmatized\n",
    "    other_corpus = [id2word.doc2bow(text) for text in other_texts]\n",
    "    unseen_doc = other_corpus[0]\n",
    "    print(unseen_doc)\n",
    "    vector = lda_model[unseen_doc]\n",
    "# get the element that has the highest score\n",
    "# this will then be the topic that fits the unseen_document the best\n",
    "    index_max = max(vector, key=itemgetter(1))\n",
    "    return index_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
      "('Score: 0.3061200678348541 Topic Index: 10 Topic: 0.049*\"order\" + '\n",
      " '0.035*\"product\" + 0.025*\"package\" + 0.023*\"item\" + 0.020*\"arrive\" + '\n",
      " '0.017*\"receive\" + 0.017*\"send\" + 0.015*\"dryer\" + 0.015*\"buy\" + 0.014*\"good\" '\n",
      " '+ 0.014*\"ship\" + 0.013*\"find\" + 0.013*\"purchase\" + 0.012*\"company\" + '\n",
      " '0.011*\"get\" + 0.010*\"toilet\" + 0.010*\"packaging\" + 0.010*\"love\" + '\n",
      " '0.009*\"online\" + 0.008*\"compliment\" + 0.008*\"really\" + 0.008*\"fruit\" + '\n",
      " '0.008*\"quickly\" + 0.007*\"pocket\" + 0.007*\"candy\" + 0.007*\"pack\" + '\n",
      " '0.007*\"always\" + 0.007*\"please\" + 0.007*\"go\" + 0.006*\"salon\"')\n"
     ]
    }
   ],
   "source": [
    "# Selected sentneces to test by uncommenting\n",
    "# unseen_document = \"Beetlejuice is a movie I consider to be one of Tim Burton's best movies. I also consider it to be one of those kind of movies that could have come out only in the 80s (much like Labyrinth starring David Bowie and Jennifer Connelly).Beetlejuice deals with a recently deceased married couple, the Maitlands, who finds themselves essentially \"'trapped'\" in their former house for the next century or so. Unfortunately, this means living with the house's new owners, the Deetz. The Maitlands don't mind Deetz daughter Lydia played by Winona Ryder so much but her much more obnoxious parents and want to scare them away from the house. Their case worker tells them only one thing\"\n",
    "# unseen_document = \"I was looking for some blue wax because I noticed that in European Wax Center usethat kind of wax and it is so less painfull. I really think that this product has the same effect. The kit is perfectly designed to make your waxing easier and with minimal pain as possible.\"\n",
    "unseen_document = \"I have been having eye irritation and issues for a while now. Every eye makeup remover burns and irritates my eyes. However, when I use jojoba oil, my make up comes off easily and there is no irritation. It is the best thing ever!!!! You can read the other reviews to see the other AMAZING benefits of it :)\"\n",
    "\n",
    "\n",
    "\n",
    "# proccess unseen_document and get the top topic and its words\n",
    "hotTopic = getTopTopic(unseen_document)\n",
    "pprint(\"Score: {} Topic Index: {} Topic: {}\".format(hotTopic[1], hotTopic[0], lda_model.print_topic(hotTopic[0], 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
