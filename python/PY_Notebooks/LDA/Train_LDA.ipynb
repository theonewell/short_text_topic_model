{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0472a605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/theo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all packages needed\n",
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b54ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "\n",
    "# gensim logging\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dc68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ad09a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train_40k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ba6e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The description and photo on this product needs to be changed to indicate '\n",
      " 'this product is the BuffalOs version of this beef jerky.']\n"
     ]
    }
   ],
   "source": [
    "# convert to list\n",
    "data = df.Text.values.tolist()\n",
    "\n",
    "# remove emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# remove single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47785b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'description', 'and', 'photo', 'on', 'this', 'product', 'needs', 'to', 'be', 'changed', 'to', 'indicate', 'this', 'product', 'is', 'the', 'buffalos', 'version', 'of', 'this', 'beef', 'jerky']]\n"
     ]
    }
   ],
   "source": [
    "# split senteces to list of words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c829f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'description', 'and', 'photo', 'on', 'this', 'product', 'needs', 'to', 'be', 'changed', 'to', 'indicate', 'this', 'product', 'is', 'the', 'buffalos', 'version', 'of', 'this', 'beef_jerky']\n"
     ]
    }
   ],
   "source": [
    "# Build bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# show trigram\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbaaf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd42dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['description', 'photo', 'product', 'need', 'change', 'indicate', 'product', 'version']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'photo', 'product', 'need', 'change', 'indicate', 'product', 'version']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79e514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4049ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e919069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "file_dir = \"LDA_Model/LDA.model\"\n",
    "lda_model.save(file_dir)\n",
    "lda_model = gensim.models.ldamodel.LdaModel.load(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13026849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.053*\"cat\" + 0.020*\"get\" + 0.019*\"box\" + 0.018*\"work\" + 0.013*\"buy\" + '\n",
      "  '0.013*\"pet\" + 0.012*\"go\" + 0.010*\"litter\" + 0.010*\"time\" + 0.009*\"day\"'),\n",
      " (1,\n",
      "  '0.026*\"price\" + 0.024*\"use\" + 0.021*\"good\" + 0.020*\"brush\" + 0.019*\"well\" + '\n",
      "  '0.018*\"great\" + 0.018*\"year\" + 0.018*\"product\" + 0.017*\"amazon\" + '\n",
      "  '0.015*\"work\"'),\n",
      " (2,\n",
      "  '0.083*\"dog\" + 0.018*\"work\" + 0.015*\"product\" + 0.014*\"get\" + 0.013*\"pain\" + '\n",
      "  '0.012*\"use\" + 0.011*\"pad\" + 0.011*\"shampoo\" + 0.011*\"treat\" + 0.011*\"try\"'),\n",
      " (3,\n",
      "  '0.016*\"fit\" + 0.013*\"easy\" + 0.012*\"get\" + 0.011*\"put\" + 0.011*\"well\" + '\n",
      "  '0.009*\"seat\" + 0.009*\"baby\" + 0.009*\"also\" + 0.009*\"little\" + 0.008*\"side\"'),\n",
      " (4,\n",
      "  '0.028*\"love\" + 0.022*\"food\" + 0.015*\"puppy\" + 0.014*\"buy\" + 0.014*\"get\" + '\n",
      "  '0.012*\"little\" + 0.011*\"make\" + 0.009*\"go\" + 0.008*\"year\" + 0.008*\"time\"'),\n",
      " (5,\n",
      "  '0.030*\"taste\" + 0.025*\"good\" + 0.019*\"bottle\" + 0.018*\"flavor\" + '\n",
      "  '0.016*\"water\" + 0.015*\"try\" + 0.014*\"eat\" + 0.014*\"product\" + 0.013*\"great\" '\n",
      "  '+ 0.013*\"well\"'),\n",
      " (6,\n",
      "  '0.036*\"baby\" + 0.033*\"bed\" + 0.030*\"sleep\" + 0.029*\"night\" + 0.029*\"love\" + '\n",
      "  '0.013*\"great\" + 0.012*\"buy\" + 0.012*\"train\" + 0.010*\"old\" + 0.010*\"look\"'),\n",
      " (7,\n",
      "  '0.032*\"old\" + 0.030*\"love\" + 0.027*\"year\" + 0.024*\"set\" + 0.021*\"toy\" + '\n",
      "  '0.018*\"great\" + 0.018*\"buy\" + 0.017*\"play\" + 0.016*\"kid\" + 0.014*\"little\"'),\n",
      " (8,\n",
      "  '0.048*\"bag\" + 0.023*\"gate\" + 0.023*\"open\" + 0.015*\"deliver\" + '\n",
      "  '0.014*\"length\" + 0.014*\"cage\" + 0.013*\"door\" + 0.012*\"get\" + 0.011*\"plant\" '\n",
      "  '+ 0.010*\"break\"'),\n",
      " (9,\n",
      "  '0.040*\"size\" + 0.030*\"great\" + 0.022*\"easy\" + 0.018*\"small\" + '\n",
      "  '0.016*\"travel\" + 0.015*\"perfect\" + 0.010*\"large\" + 0.009*\"mild\" + '\n",
      "  '0.009*\"good\" + 0.008*\"poop\"'),\n",
      " (10,\n",
      "  '0.049*\"order\" + 0.035*\"product\" + 0.025*\"package\" + 0.023*\"item\" + '\n",
      "  '0.020*\"arrive\" + 0.017*\"receive\" + 0.017*\"send\" + 0.015*\"dryer\" + '\n",
      "  '0.015*\"buy\" + 0.014*\"good\"'),\n",
      " (11,\n",
      "  '0.053*\"game\" + 0.039*\"play\" + 0.030*\"piece\" + 0.024*\"fun\" + 0.018*\"puzzle\" '\n",
      "  '+ 0.016*\"card\" + 0.011*\"kid\" + 0.011*\"get\" + 0.010*\"great\" + 0.010*\"make\"'),\n",
      " (12,\n",
      "  '0.029*\"skin\" + 0.020*\"product\" + 0.020*\"smell\" + 0.019*\"use\" + 0.012*\"feel\" '\n",
      "  '+ 0.011*\"cream\" + 0.011*\"soap\" + 0.010*\"make\" + 0.010*\"face\" + '\n",
      "  '0.009*\"strong\"'),\n",
      " (13,\n",
      "  '0.060*\"wear\" + 0.041*\"scent\" + 0.031*\"fragrance\" + 0.030*\"filter\" + '\n",
      "  '0.022*\"favorite\" + 0.021*\"good\" + 0.020*\"love\" + 0.019*\"smell\" + '\n",
      "  '0.017*\"woman\" + 0.015*\"shoe\"'),\n",
      " (14,\n",
      "  '0.055*\"hair\" + 0.032*\"product\" + 0.019*\"use\" + 0.017*\"tea\" + 0.013*\"good\" + '\n",
      "  '0.012*\"well\" + 0.012*\"get\" + 0.011*\"buy\" + 0.011*\"dry\" + 0.011*\"really\"'),\n",
      " (15,\n",
      "  '0.079*\"toy\" + 0.030*\"love\" + 0.025*\"sound\" + 0.021*\"ball\" + 0.020*\"month\" + '\n",
      "  '0.020*\"play\" + 0.020*\"old\" + 0.017*\"get\" + 0.016*\"son\" + 0.013*\"tank\"'),\n",
      " (16,\n",
      "  '0.014*\"get\" + 0.013*\"love\" + 0.013*\"lip\" + 0.012*\"stuff\" + 0.011*\"period\" + '\n",
      "  '0.011*\"really\" + 0.010*\"walk\" + 0.009*\"tasty\" + 0.009*\"time\" + '\n",
      "  '0.008*\"year\"'),\n",
      " (17,\n",
      "  '0.013*\"get\" + 0.011*\"year\" + 0.011*\"read\" + 0.010*\"find\" + 0.010*\"bark\" + '\n",
      "  '0.009*\"know\" + 0.009*\"time\" + 0.007*\"comb\" + 0.007*\"purchase\" + '\n",
      "  '0.006*\"even\"'),\n",
      " (18,\n",
      "  '0.042*\"color\" + 0.028*\"look\" + 0.016*\"make\" + 0.012*\"really\" + '\n",
      "  '0.011*\"light\" + 0.010*\"nice\" + 0.010*\"oil\" + 0.010*\"love\" + 0.008*\"great\" + '\n",
      "  '0.008*\"little\"'),\n",
      " (19,\n",
      "  '0.023*\"product\" + 0.019*\"take\" + 0.017*\"get\" + 0.016*\"day\" + 0.015*\"work\" + '\n",
      "  '0.013*\"time\" + 0.013*\"try\" + 0.011*\"go\" + 0.011*\"use\" + 0.010*\"first\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
