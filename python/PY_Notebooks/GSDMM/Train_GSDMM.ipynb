{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b54ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/theo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# KeyBERT\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "\n",
    "# gensim logging\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2dc68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad09a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train_40k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ba6e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The description and photo on this product needs to be changed to indicate '\n",
      " 'this product is the BuffalOs version of this beef jerky.']\n"
     ]
    }
   ],
   "source": [
    "# convert to list\n",
    "data = df.Text.values.tolist()\n",
    "\n",
    "# remove emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# remove single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47785b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'description', 'and', 'photo', 'on', 'this', 'product', 'needs', 'to', 'be', 'changed', 'to', 'indicate', 'this', 'product', 'is', 'the', 'buffalos', 'version', 'of', 'this', 'beef', 'jerky']]\n"
     ]
    }
   ],
   "source": [
    "# split senteces to list of words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c829f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'description', 'and', 'photo', 'on', 'this', 'product', 'needs', 'to', 'be', 'changed', 'to', 'indicate', 'this', 'product', 'is', 'the', 'buffalos', 'version', 'of', 'this', 'beef_jerky']\n"
     ]
    }
   ],
   "source": [
    "# Build bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# show trigram\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbaaf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords, bigrams, trigrams, lemmatization functions\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd42dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['description', 'photo', 'product', 'need', 'change', 'indicate', 'product', 'version']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Make Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8e3caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['description', 'photo', 'product', 'need', 'change', 'indicate', 'product', 'version']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52424007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33665 unique tokens: ['change', 'description', 'indicate', 'need', 'photo']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary_sample = gensim.corpora.Dictionary(data_lemmatized)\n",
    "print(dictionary_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f89e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vocab_length = len(dictionary_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45ec704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 38405 clusters with 50 clusters populated\n",
      "In stage 1: transferred 21353 clusters with 50 clusters populated\n",
      "In stage 2: transferred 9535 clusters with 46 clusters populated\n",
      "In stage 3: transferred 6894 clusters with 39 clusters populated\n",
      "In stage 4: transferred 5713 clusters with 35 clusters populated\n",
      "In stage 5: transferred 4814 clusters with 35 clusters populated\n",
      "In stage 6: transferred 4297 clusters with 34 clusters populated\n",
      "In stage 7: transferred 4028 clusters with 34 clusters populated\n",
      "In stage 8: transferred 4003 clusters with 34 clusters populated\n",
      "In stage 9: transferred 3916 clusters with 33 clusters populated\n"
     ]
    }
   ],
   "source": [
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=50, alpha=0.1, beta=0.1, n_iters=10)\n",
    "\n",
    "y = gsdmm.fit(data_lemmatized, sample_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df176edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 12 : [('toy', 4933), ('love', 4690), ('play', 4208), ('old', 4153), ('get', 3444), ('game', 3005), ('year', 2847), ('buy', 2830), ('great', 2774), ('son', 2211), ('fun', 2040), ('make', 2000), ('time', 1994), ('little', 1973), ('daughter', 1921), ('month', 1816), ('well', 1627), ('kid', 1622), ('really', 1615), ('good', 1553)]\n",
      "\n",
      "Cluster 49 : [('get', 3345), ('use', 2437), ('make', 2430), ('product', 2401), ('time', 2330), ('work', 2267), ('well', 2186), ('go', 1943), ('good', 1832), ('take', 1791), ('buy', 1786), ('also', 1623), ('try', 1615), ('little', 1468), ('first', 1442), ('day', 1435), ('even', 1433), ('really', 1414), ('find', 1406), ('much', 1390)]\n",
      "\n",
      "Cluster 28 : [('get', 1213), ('buy', 1081), ('well', 1064), ('great', 1036), ('product', 958), ('make', 895), ('love', 877), ('use', 822), ('easy', 760), ('bed', 753), ('good', 742), ('put', 720), ('work', 709), ('baby', 666), ('also', 636), ('fit', 635), ('dog', 635), ('go', 631), ('little', 626), ('look', 622)]\n",
      "\n",
      "Cluster 34 : [('taste', 1720), ('good', 1605), ('flavor', 1170), ('product', 949), ('great', 941), ('make', 871), ('bar', 847), ('try', 846), ('well', 788), ('love', 721), ('buy', 703), ('find', 696), ('eat', 677), ('get', 641), ('really', 505), ('go', 485), ('use', 470), ('much', 457), ('mix', 452), ('little', 446)]\n",
      "\n",
      "Cluster 20 : [('product', 1788), ('order', 1007), ('buy', 894), ('price', 839), ('amazon', 839), ('get', 758), ('great', 731), ('good', 716), ('find', 606), ('item', 585), ('time', 564), ('receive', 532), ('purchase', 530), ('well', 434), ('work', 415), ('love', 381), ('ship', 363), ('send', 358), ('arrive', 352), ('store', 347)]\n",
      "\n",
      "Cluster 42 : [('use', 1161), ('get', 976), ('work', 905), ('well', 902), ('good', 837), ('product', 825), ('shave', 795), ('buy', 778), ('great', 722), ('time', 696), ('hair', 676), ('year', 675), ('battery', 615), ('razor', 563), ('shaver', 549), ('brush', 492), ('go', 452), ('blade', 446), ('make', 441), ('long', 436)]\n",
      "\n",
      "Cluster 46 : [('product', 2138), ('skin', 1548), ('use', 1436), ('try', 772), ('work', 753), ('well', 750), ('great', 735), ('get', 700), ('good', 679), ('make', 673), ('love', 625), ('smell', 615), ('really', 592), ('feel', 553), ('dry', 550), ('face', 546), ('buy', 526), ('year', 520), ('day', 509), ('go', 493)]\n",
      "\n",
      "Cluster 8 : [('product', 1503), ('take', 1389), ('work', 878), ('day', 871), ('use', 761), ('get', 734), ('try', 612), ('well', 601), ('good', 539), ('year', 523), ('time', 502), ('help', 485), ('feel', 476), ('go', 421), ('great', 420), ('really', 399), ('make', 397), ('give', 381), ('find', 379), ('month', 378)]\n",
      "\n",
      "Cluster 13 : [('hair', 3038), ('product', 1045), ('color', 818), ('use', 791), ('get', 623), ('look', 603), ('good', 556), ('great', 554), ('work', 528), ('buy', 510), ('love', 509), ('well', 507), ('make', 484), ('dry', 450), ('really', 432), ('try', 379), ('time', 376), ('long', 373), ('year', 331), ('find', 318)]\n",
      "\n",
      "Cluster 30 : [('baby', 1043), ('bottle', 1022), ('diaper', 874), ('use', 793), ('get', 680), ('time', 576), ('well', 568), ('work', 552), ('great', 537), ('buy', 529), ('pump', 510), ('love', 494), ('leak', 479), ('good', 449), ('try', 421), ('month', 414), ('go', 394), ('son', 394), ('tub', 386), ('much', 371)]\n",
      "\n",
      "Cluster 38 : [('get', 612), ('look', 542), ('doll', 535), ('toy', 478), ('good', 447), ('figure', 403), ('great', 392), ('make', 380), ('well', 363), ('buy', 357), ('really', 319), ('come', 309), ('think', 308), ('love', 287), ('fun', 263), ('play', 247), ('go', 233), ('cool', 228), ('also', 218), ('set', 208)]\n",
      "\n",
      "Cluster 48 : [('dog', 1261), ('toy', 1025), ('love', 694), ('cat', 633), ('get', 548), ('food', 419), ('treat', 409), ('buy', 372), ('chew', 337), ('good', 325), ('great', 309), ('ball', 293), ('play', 285), ('go', 274), ('make', 273), ('time', 273), ('well', 271), ('product', 252), ('small', 246), ('little', 240)]\n",
      "\n",
      "Cluster 47 : [('smell', 877), ('scent', 689), ('love', 481), ('fragrance', 439), ('wear', 410), ('perfume', 407), ('good', 312), ('get', 293), ('buy', 289), ('cologne', 262), ('great', 246), ('time', 238), ('year', 221), ('last', 215), ('bottle', 213), ('find', 212), ('day', 192), ('product', 189), ('go', 175), ('use', 168)]\n",
      "\n",
      "Cluster 4 : [('stroller', 606), ('seat', 541), ('get', 384), ('love', 378), ('easy', 372), ('bag', 365), ('great', 356), ('car_seat', 340), ('old', 331), ('baby', 317), ('buy', 287), ('fit', 279), ('well', 258), ('use', 252), ('go', 245), ('make', 242), ('also', 215), ('good', 205), ('take', 205), ('month', 200)]\n",
      "\n",
      "Cluster 24 : [('tea', 1011), ('taste', 324), ('coffee', 296), ('good', 295), ('flavor', 277), ('drink', 183), ('make', 178), ('buy', 162), ('find', 157), ('love', 154), ('try', 154), ('bag', 152), ('great', 144), ('well', 135), ('get', 130), ('green', 101), ('cup', 96), ('strong', 95), ('really', 93), ('time', 93)]\n"
     ]
    }
   ],
   "source": [
    "top_words(gsdmm.cluster_word_distribution, top_index, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba63102",
   "metadata": {},
   "source": [
    "<h1>Predict Unseen<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f3ca4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentence):\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f727dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12b8685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8338e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)  \n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e484b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "198ebb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatUnseen(unseen_document):\n",
    "    data_words = list(sent_to_words(unseen_document))\n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(sent_to_words(data_words))\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    return data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3728c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def getTopTopic(unseen_document):\n",
    "    data_lemmatized = formatUnseen(unseen_document)\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\n",
    "    # Create Corpus\n",
    "    other_texts = data_lemmatized\n",
    "    other_corpus = [id2word.doc2bow(text) for text in other_texts]\n",
    "    unseen_doc = other_corpus[0]\n",
    "    vector = gsdmm.score(unseen_doc)\n",
    "    print(vector)\n",
    "    # get the element that has the highest score\n",
    "# this will then be the topic that fits the unseen_document the best\n",
    "    index = vector.index(max(vector))\n",
    "    print(vector[index])\n",
    "    print(index)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2abf2e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06231680932669086, 0.003907617835293337, 0.18917573328514292, 0.003907617835293337, 9.989724884871218e-18, 0.003907617835293337, 0.003907617835293337, 0.039080130600877644, 2.2312759901962293e-22, 0.003907617835293337, 7.875480012040694e-12, 2.5204938812069545e-05, 4.5610112014628927e-32, 3.001715627168319e-21, 0.010329735082790326, 0.003907617835293337, 0.11851480313599229, 0.05670912194869464, 0.09504155027882039, 0.003907617835293337, 1.296960965770374e-21, 0.003907617835293337, 1.2093618915470952e-11, 0.14427686892640004, 3.0755539584471496e-13, 0.04988920934776385, 1.527125454854196e-09, 0.003907617835293337, 6.287717567829975e-26, 0.06359817234815798, 1.3084113173646442e-21, 0.003907617835293337, 0.10461251079859575, 0.003907617835293337, 2.454459013185963e-24, 2.4648783437525203e-11, 0.003907617835293337, 0.003907617835293337, 2.317472953263218e-19, 9.048905837444215e-08, 0.003907617835293337, 0.003907617835293337, 2.3832051095301657e-23, 0.003907617835293337, 5.54720165517709e-07, 0.003907617835293337, 1.9387997645636904e-23, 2.0168735727671754e-16, 6.989454639909955e-19, 3.1541904126808445e-34]\n",
      "0.18917573328514292\n",
      "2\n",
      "\n",
      "Cluster 2 : [('iacute', 15), ('que', 13), ('por', 10), ('aacute', 8), ('este', 7), ('excelente', 5), ('bend', 4), ('con', 4), ('sin', 4), ('optical', 4), ('recarga', 3), ('debe', 3), ('todo', 3), ('ntilde', 3), ('para', 3), ('ante', 3), ('mucho', 3), ('asi', 3), ('art', 3), ('buy', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Selected sentneces to test by uncommenting\n",
    "# unseen_document = \"Beetlejuice is a movie I consider to be one of Tim Burton's best movies. I also consider it to be one of those kind of movies that could have come out only in the 80s (much like Labyrinth starring David Bowie and Jennifer Connelly).Beetlejuice deals with a recently deceased married couple, the Maitlands, who finds themselves essentially \"'trapped'\" in their former house for the next century or so. Unfortunately, this means living with the house's new owners, the Deetz. The Maitlands don't mind Deetz daughter Lydia played by Winona Ryder so much but her much more obnoxious parents and want to scare them away from the house. Their case worker tells them only one thing\"\n",
    "# unseen_document = \"I was looking for some blue wax because I noticed that in European Wax Center usethat kind of wax and it is so less painfull. I really think that this product has the same effect. The kit is perfectly designed to make your waxing easier and with minimal pain as possible.\"\n",
    "unseen_document = \"I have been having eye irritation and issues for a while now. Every eye makeup remover burns and irritates my eyes. However, when I use jojoba oil, my make up comes off easily and there is no irritation. It is the best thing ever!!!! You can read the other reviews to see the other AMAZING benefits of it :)\"\n",
    "# unseen_document = \"I am a very picky person when it comes to men's cologne and women's perfumes. I only use Herrera. Nothing comes close to Herrera for Men. It has a very unique smell and isn't overwhelming. It's sure to set yourself apart from the traditional guy cologne's and old man musks out there. Everyone always wants to know what I am wearing because it is so unique and pleasing to smell. Try it, you'll love it and so will your lady, period.\"\n",
    "# unseen_document = \"This is a great product at a great price. I have been using a Waterpik for years and love it.\"\n",
    "\n",
    "# proccess unseen_document and get the top topic and its words\n",
    "hotTopic = getTopTopic(unseen_document)\n",
    "top_words(gsdmm.cluster_word_distribution, [hotTopic], 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
